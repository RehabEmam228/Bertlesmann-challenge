{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNISTdigits_by_nn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONV1RUGw944sduPkpiBkcA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RehabEmam228/Bertlesmann-challenge/blob/master/MNISTdigits_by_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8aLY_4kn-pg",
        "colab_type": "text"
      },
      "source": [
        "## 4 Ways to created models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERWmOqBSfOtB",
        "colab_type": "text"
      },
      "source": [
        "## 1- Using nn to create a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUuGMrAfcRTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.hidden = nn.Linear(784, 256)\n",
        "    self.output = nn.Linear(256, 10)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "  def forward(self, x):\n",
        "    x = self.hidden(x)\n",
        "    x = self.sigmoid(x)\n",
        "    x = self.output(x)\n",
        "    x = self.softmax(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaLbdA4ve_sW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "6bc67c88-ca32-4e3d-a8cc-bfd740121558"
      },
      "source": [
        "model = Network()\n",
        "model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
              "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt1gwuGafcpg",
        "colab_type": "text"
      },
      "source": [
        "## 2- Using nn.functional to create a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cr_OdxHfpB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.hidden = nn.Linear(784, 256)\n",
        "    self.output = nn.Linear(256, 10)\n",
        "  def forward(self, x):\n",
        "    x = F.sigmoid(self.hidden(x))\n",
        "    x = F.softmax(self.output(x), dim=1)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z40oK3phSUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "43a89efc-4d50-4dfe-89ac-4e25860dbc65"
      },
      "source": [
        "model = Network()\n",
        "model"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
              "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYeKmBnHhu0z",
        "colab_type": "text"
      },
      "source": [
        "## Activation Functions\n",
        "# ReLU function is used almost exclusively as the activation function for hidden layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3AzLD5GiEv2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "6134fabb-9b72-4a9a-e46d-dbfaefc2e486"
      },
      "source": [
        "# Creating a model with 2 hidden(fully connected) layers\n",
        "import torch.nn.functional as F\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10)\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.softmax(x, dim=1)\n",
        "    return x\n",
        "\n",
        "model = Network()\n",
        "model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPK1ANm9kUaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "1738ddc8-5ba1-45b9-c8a5-8185b5033121"
      },
      "source": [
        "# I can access each layer's weights and bias tensors\n",
        "print(model.fc1.weight)\n",
        "print(model.fc1.bias)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0090, -0.0095, -0.0281,  ..., -0.0209, -0.0106, -0.0254],\n",
            "        [ 0.0157,  0.0025,  0.0116,  ..., -0.0101, -0.0077, -0.0356],\n",
            "        [ 0.0155,  0.0112,  0.0287,  ...,  0.0291, -0.0045,  0.0350],\n",
            "        ...,\n",
            "        [-0.0170, -0.0345,  0.0134,  ...,  0.0154,  0.0285,  0.0237],\n",
            "        [ 0.0113, -0.0140,  0.0262,  ..., -0.0028,  0.0271,  0.0012],\n",
            "        [-0.0120,  0.0327,  0.0341,  ..., -0.0316,  0.0341, -0.0338]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0086, -0.0134, -0.0186,  0.0204, -0.0089,  0.0252,  0.0209,  0.0192,\n",
            "        -0.0309, -0.0055,  0.0266, -0.0229, -0.0343, -0.0256,  0.0102, -0.0116,\n",
            "        -0.0151, -0.0032, -0.0057, -0.0268,  0.0211,  0.0252, -0.0188,  0.0262,\n",
            "         0.0099, -0.0109,  0.0093, -0.0076, -0.0071, -0.0189, -0.0180,  0.0219,\n",
            "         0.0166, -0.0269, -0.0055, -0.0328, -0.0002,  0.0269, -0.0330,  0.0121,\n",
            "         0.0352,  0.0196, -0.0181, -0.0064, -0.0166,  0.0334,  0.0295, -0.0173,\n",
            "        -0.0217,  0.0111,  0.0339, -0.0128,  0.0170,  0.0043,  0.0063,  0.0251,\n",
            "         0.0279,  0.0061,  0.0286,  0.0062,  0.0287,  0.0123,  0.0336,  0.0278,\n",
            "        -0.0212,  0.0276,  0.0301,  0.0071,  0.0073, -0.0238,  0.0282,  0.0166,\n",
            "         0.0045, -0.0276, -0.0156, -0.0141, -0.0038, -0.0133,  0.0230, -0.0266,\n",
            "        -0.0097, -0.0312,  0.0200,  0.0042, -0.0104,  0.0295, -0.0103, -0.0351,\n",
            "         0.0120,  0.0186,  0.0225,  0.0084,  0.0202,  0.0354, -0.0237,  0.0289,\n",
            "         0.0326, -0.0229, -0.0293, -0.0198, -0.0168,  0.0233,  0.0169,  0.0255,\n",
            "         0.0123, -0.0074,  0.0006,  0.0309, -0.0044,  0.0016,  0.0205, -0.0257,\n",
            "        -0.0248, -0.0340, -0.0237,  0.0326,  0.0311, -0.0211, -0.0001,  0.0225,\n",
            "        -0.0192,  0.0166, -0.0256, -0.0262,  0.0090, -0.0356, -0.0265,  0.0315],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEepLM0rlue2",
        "colab_type": "text"
      },
      "source": [
        "## 3- Building a model using nn.sequential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWF2ZMtDl7Ft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "143a59ac-ec47-465f-bcf0-f06122e18144"
      },
      "source": [
        "input_size = 784\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 10\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_sizes[0]),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_sizes[1], output_size),\n",
        "    nn.Softmax(dim=1)\n",
        "\n",
        ")\n",
        "print(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (5): Softmax(dim=1)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7WzCV0God8N",
        "colab_type": "text"
      },
      "source": [
        "## 4- Using OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fILhQ44xooT6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "e091a06b-0f59-4e80-f97b-d9e807bba21d"
      },
      "source": [
        "# Dictionaries keys are unique, so it's important to have different name to each key\n",
        "from collections import OrderedDict\n",
        "model = nn.Sequential(OrderedDict([\n",
        "                                   ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
        "                                   ('relu1', nn.ReLU()),\n",
        "                                   ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
        "                                   ('relu2', nn.ReLU()),\n",
        "                                   ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
        "                                   ('softmax', nn.Softmax(dim=1))\n",
        "]))\n",
        "print(model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}